<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>爬虫爬取网页内外链 | OnnekeS</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="用Beautifulsoup库写了一个小爬虫，用于爬取一个网站所有的内链和外链。">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫爬取网页内外链">
<meta property="og:url" content="https://dogger123.github.io/2017/03/21/爬虫爬取网页内外链/index.html">
<meta property="og:site_name" content="OnnekeS">
<meta property="og:description" content="用Beautifulsoup库写了一个小爬虫，用于爬取一个网站所有的内链和外链。">
<meta property="og:updated_time" content="2017-03-21T09:03:48.873Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="爬虫爬取网页内外链">
<meta name="twitter:description" content="用Beautifulsoup库写了一个小爬虫，用于爬取一个网站所有的内链和外链。">
<meta name="twitter:creator" content="@https://twitter.com/">
  
    <link rel="alternate" href="/atom.xml" title="OnnekeS" type="application/atom+xml">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">OnnekeS</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="Flux RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Rechercher"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://dogger123.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-爬虫爬取网页内外链" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/03/21/爬虫爬取网页内外链/" class="article-date">
  <time datetime="2017-03-21T08:36:29.000Z" itemprop="datePublished">2017-03-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      爬虫爬取网页内外链
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>用Beautifulsoup库写了一个小爬虫，用于爬取一个网站所有的内链和外链。</p>
</blockquote>
<a id="more"></a>
<p>首先我先解释一下什么是内链和外链，内链是一个网站中指向自身文件的链接，他的URL一般为 <strong>../..</strong>,而外链，通俗的说就是我们平常浏览网页浏览器所用的URL，一般格式为 <strong>'www','http','https'</strong>。<br>
上面的两种格式是我们区分内链和外链的重要条件，我们将用正则表达式将他们加以区分和删选。<br>
下面就是完整的代码：</p>
<pre><code class="language-python">    #encoding:utf-8
    from urllib import urlopen
    from bs4 import BeautifulSoup
    import re
    import datetime
    import random
    pages = set()
    random.seed(datetime.datetime.now())
    # 获取页面所有内链的列表
    def getInternalLinks(bsObj, includeUrl):
        internalLinks = []
        # 找出所有以&quot;/&quot;开头的链接
        for link in bsObj.findAll(&quot;a&quot;, href=re.compile(&quot;^(/|.*&quot;+includeUrl+&quot;)&quot;)):
            if link.attrs['href'] is not None:
                if link.attrs['href'] not in internalLinks:
                    internalLinks.append(link.attrs['href'])
        return internalLinks
    # 获取页面所有外链的列表
    def getExternalLinks(bsObj, excludeUrl):
        externalLinks = []
    # 找出所有以&quot;http&quot;或&quot;www&quot;开头且不包含当前URL的链接
        for link in bsObj.findAll(&quot;a&quot;,href=re.compile(&quot;^(http|www)((?!&quot;+excludeUrl+&quot;).)*$&quot;)):
            if link.attrs['href'] is not None:
                if link.attrs['href'] not in externalLinks:
                    externalLinks.append(link.attrs['href'])
        return externalLinks
    def splitAddress(address):
        addressParts = address.replace(&quot;http://&quot;, &quot;&quot;).split(&quot;/&quot;)
        return addressParts

    def getRandomExternalLink(startingPage):
        html = urlopen(startingPage)
        bsObj = BeautifulSoup(html)
        externalLinks = getExternalLinks(bsObj, splitAddress(startingPage)[0])
        if len(externalLinks) == 0:
            internalLinks = getInternalLinks(startingPage)
            return getExternalLinks(internalLinks[random.randint(0,len(internalLinks)-1)])
        else:
            return externalLinks[random.randint(0, len(externalLinks)-1)]

    def followExternalOnly(startingSite):
        externalLink = getRandomExternalLink(&quot;https://www.luckyye.com/&quot;)
        print(&quot;随机外链是：&quot;+externalLink)
        followExternalOnly(externalLink)

    followExternalOnly(&quot;https://www.luckyye.com/&quot;)
</code></pre>
<p>运行这段代码之后，我试了不同的网站，很有意思的是，我输入了一个我经常逛的论坛V2EX的地址，虽然代码成功了，但是在浏览器上访问这个网站时，显示 <strong>“ASSCE DENIAL”</strong> 禁止访问，看来是我的爬虫引起了网站的反爬机制，但一段时间后，又能进行访问了。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://dogger123.github.io/2017/03/21/爬虫爬取网页内外链/" data-id="cj0jbk5ns0006ukuaeg2yxjdv" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2017/02/28/python爬虫初体验/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Ancien</strong>
      <div class="article-nav-title">python爬虫初体验</div>
    </a>
  
</nav>

  
</article>

</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 XuJunhao<br>
      Propulsé by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>



<script src="/js/script.js"></script>

  </div>
</body>
</html>